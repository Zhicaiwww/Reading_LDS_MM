# Reading\_LDS_MM
This is a list of papers shared in LDS multi_media Group
## Table of contents
- [Survey Paper](#1)
- [Model and Architecture](#2)
- [Causality in Computer Vision](#3)
- [Video Representation Learning](#4)
- [Multi-Modal](#5)


## <h2 id="1">Survey Paper</h2>
- [Generative Adversarial Networks for Image and Video Synthesis: Algorithms and Applications](http://arxiv.org/abs/2008.02793)(2020)
- [Cross-modal interaction networks for query-based moment retrieval in videos](http://arxiv.org/abs/1607.06215)(2019SIGIR)

## <h2 id="2">Model and Architecture</h2>
- [V4D:4D convolutional neural networks for video-level representation learning](http://arxiv.org/abs/2002.07442)(2020)
- [Non-local Neural Networks](http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Non-Local_Neural_Networks_CVPR_2018_paper.pdf)(2018CVPR)
- [X3D: Expanding Architectures for Efficient Video Recognition](http://arxiv.org/abs/2004.04730)(2020)
- [Selective kernel networks](https://openaccess.thecvf.com/content_CVPR_2019/html/Li_Selective_Kernel_Networks_CVPR_2019_paper.html)(2019CVPR)
- [SpeedNet: Learning the Speediness in Videos](http://arxiv.org/abs/2004.06130)(2020)
- [BAM: Bottleneck attention module](http://arxiv.org/abs/1807.06514)(2018)
- [RubiksNet: Learnable 3D-Shift for Efficient Video Action Recognition](https://rubiksnet.stanford.edu/)(2020ECCV)
- [Cbam: Convolutional block attention module](https://openaccess.thecvf.com/content_ECCV_2018/html/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.html)(2018ECCV)
- [Gate-shift networks for video action recognition](https://openaccess.thecvf.com/content_CVPR_2020/html/Sudhakaran_Gate-Shift_Networks_for_Video_Action_Recognition_CVPR_2020_paper.html)(2020CVPR)
- [Smallbignet: Integrating core and contextual views for video classification](https://arxiv.org/abs/2006.14582)(2020)
- [Rotate to Attend: Convolutional triplet attention module](http://arxiv.org/abs/2010.03045)(2020)
- [ON THE RELATIONSHIP BETWEEN SELF-ATTENTION AND CONVOLUTIONAL LAYERS](https://arxiv.org/abs/1911.03584)(2020ICLR)
- [ConViT : Improving Vision Transformers with Soft Convolutional Inductive Biases](http://arxiv.org/abs/2103.10697)(2021ICML)

## <h2 id="3">Causality in Computer Vision</h2>
- [The Blessings of Unlabeled Background in Untrimmed Videos](http://arxiv.org/abs/2103.13183)(2021)
- [The Blessings of Multiple Causes](https://arxiv.org/abs/1805.06826)(2019)
- [Causal Attention for Vision-Language Tasks](https://arxiv.org/abs/2103.03493)(2021)
- [Interventional Few-Shot Learning](https://arxiv.org/abs/2009.13000)(2020NeurIPS)
- [Interventional Video Grounding With Dual Contrastive Learning](https://openaccess.thecvf.com/content/CVPR2021/html/Nan_Interventional_Video_Grounding_With_Dual_Contrastive_Learning_CVPR_2021_paper.html)(2021CVPR)
- [Counterfactual Samples Synthesizing for Robust Visual Question Answering](https://openaccess.thecvf.com/content_CVPR_2020/html/Chen_Counterfactual_Samples_Synthesizing_for_Robust_Visual_Question_Answering_CVPR_2020_paper.html)(2020CVPR) 
## <h2 id="4">Video Representation Learning</h2>
- [Video Representation Learning by Recognizing Temporal Transformations](http://arxiv.org/abs/2007.10730)(2020ECCV)
- [Evolving losses for unsupervised video representation learning](https://openaccess.thecvf.com/content_CVPR_2020/html/Piergiovanni_Evolving_Losses_for_Unsupervised_Video_Representation_Learning_CVPR_2020_paper.html)(2020CVPR)
- [Memory-Augmented Dense Predictive Coding for Video Representation Learning](http://arxiv.org/abs/2008.01065)(2020)
- [Self-Supervised Video Hashing via Bidirectional Transformers](https://openaccess.thecvf.com/content/CVPR2021/html/Li_Self-Supervised_Video_Hashing_via_Bidirectional_Transformers_CVPR_2021_paper.html)(2021CVPR)
- [Removing the Background by Adding the Background: Towards Background
Robust Self-supervised Video Representation Learning](https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Removing_the_Background_by_Adding_the_Background_Towards_Background_Robust_CVPR_2021_paper.html)(2021CVPR)

## <h2 id="5">Multi-Model</h2>
- [Large-Scale Few-Shot Learning via Multi-modal Knowledge Discovery](https://link.springer.com/chapter/10.1007/978-3-030-58607-2_42)(2020ECCV)
- [Audio-Visual Instance Discrimination with Cross-Modal Agreement](http://arxiv.org/abs/2004.12943)(2020)
- [Cross-modal interaction networks for query-based moment retrieval in videos](http://arxiv.org/abs/1906.02497)(2019SIGIR)
